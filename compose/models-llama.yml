services:

  vllm-llama-3.3-70b-instruct-fp4:
    profiles: ["models"]
    image: avarok/vllm-dgx-spark:v11
    container_name: vllm-llama-3.3-70b-instruct-fp4
    command:
      - serve
      - --model
      - nvidia/Llama-3.3-70B-Instruct-NVFP4
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --download-dir
      - /models
      - --served-model-name
      - llama-3.3-70b-instruct-fp4
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --gpu-memory-utilization
      - "0.82"
      - --dtype
      - auto
      - --max-model-len
      - "131072"
      - --quantization
      - modelopt_fp4
      - --kv-cache-dtype
      - fp8
      - --disable-log-requests
      - --disable-log-stats
    volumes:
      - ../vllm_cache_huggingface:/root/.cache/huggingface
      - ../models:/models
      - ../flashinfer_cache:/root/.cache/flashinfer
      - ../torch_extensions:/root/.cache/torch_extensions
      - ../torchinductor:/tmp/torchinductor_root
    environment:
      HF_HOME: /root/.cache/huggingface
      VLLM_API_KEY: ${VLLM_API_KEY:-63TestTOKEN0REPLACEME}
      VLLM_NO_USAGE_STATS: "1"
      VLLM_FLASHINFER_MOE_BACKEND: "latency"
      VLLM_CONFIGURE_LOGGING: ${VLLM_LOGGING:-0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: "all"
              capabilities: ["gpu"]
    shm_size: "16g"
    ulimits:
      memlock: -1
      stack: 67108864
    restart: "no"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 1800s
    logging:
      driver: ${DOCKER_LOG_DRIVER:-json-file}
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - default
      - vllm_internal

  vllm-llama-3.3-70b-joyous-fp4:
    profiles: ["models"]
    image: avarok/vllm-dgx-spark:v11
    container_name: vllm-llama-3.3-70b-joyous-fp4
    command:
      - serve
      - --model
      - Firworks/Llama-3.3-70B-Joyous-nvfp4
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --download-dir
      - /models
      - --served-model-name
      - llama-3.3-70b-joyous-fp4
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --gpu-memory-utilization
      - "0.82"
      - --dtype
      - auto
      - --max-model-len
      - "131072"
      - --quantization
      - compressed-tensors
      - --kv-cache-dtype
      - fp8
      - --disable-log-requests
      - --disable-log-stats
    volumes:
      - ../vllm_cache_huggingface:/root/.cache/huggingface
      - ../models:/models
      - ../flashinfer_cache:/root/.cache/flashinfer
      - ../torch_extensions:/root/.cache/torch_extensions
      - ../torchinductor:/tmp/torchinductor_root
    environment:
      HF_HOME: /root/.cache/huggingface
      VLLM_API_KEY: ${VLLM_API_KEY:-63TestTOKEN0REPLACEME}
      VLLM_NO_USAGE_STATS: "1"
      VLLM_FLASHINFER_MOE_BACKEND: "latency"
      VLLM_CONFIGURE_LOGGING: ${VLLM_LOGGING:-0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: "all"
              capabilities: ["gpu"]
    shm_size: "16g"
    ulimits:
      memlock: -1
      stack: 67108864
    restart: "no"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 1800s
    logging:
      driver: ${DOCKER_LOG_DRIVER:-json-file}
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - default
      - vllm_internal

  vllm-eurollm-22b-fp4:
    profiles: ["models"]
    image: avarok/vllm-dgx-spark:v11
    container_name: vllm-eurollm-22b-fp4
    command:
      - serve
      - --model
      - Firworks/EuroLLM-22B-Instruct-2512-nvfp4
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --download-dir
      - /models
      - --served-model-name
      - eurollm-22b-instruct-fp4
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --gpu-memory-utilization
      - "0.82"
      - --dtype
      - auto
      - --max-model-len
      - "32768"
      - --quantization
      - compressed-tensors
      - --kv-cache-dtype
      - fp8
      - --disable-log-requests
      - --disable-log-stats
    volumes:
      - ../vllm_cache_huggingface:/root/.cache/huggingface
      - ../models:/models
      - ../flashinfer_cache:/root/.cache/flashinfer
      - ../torch_extensions:/root/.cache/torch_extensions
      - ../torchinductor:/tmp/torchinductor_root
    environment:
      HF_HOME: /root/.cache/huggingface
      VLLM_API_KEY: ${VLLM_API_KEY:-63TestTOKEN0REPLACEME}
      VLLM_NO_USAGE_STATS: "1"
      VLLM_FLASHINFER_MOE_BACKEND: "latency"
      VLLM_CONFIGURE_LOGGING: ${VLLM_LOGGING:-0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: "all"
              capabilities: ["gpu"]
    shm_size: "16g"
    ulimits:
      memlock: -1
      stack: 67108864
    restart: "no"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 1800s
    logging:
      driver: ${DOCKER_LOG_DRIVER:-json-file}
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - default
      - vllm_internal

