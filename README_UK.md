# DGX Spark Inference Stack - Служіння дому!

> **Відмова від відповідальності:** Цей документ був перекладений штучним інтелектом і може містити помилки.

Ваш Nvidia DGX Spark не повинен бути ще одним побічним проектом. Почніть використовувати його! Це стек для інференсу на основі Docker для обслуговування великих мовних моделей (LLM) з використанням NVIDIA vLLM з інтелектуальним керуванням ресурсами. Цей стек забезпечує завантаження моделей на вимогу з автоматичним вимкненням при простої, однокористувацьке планування GPU та єдиний API-шлюз.

Мета проекту — надати сервер інференсу для вашого дому. Після тестування та додавання нових моделей протягом місяця я вирішив випустити його для спільноти. Будь ласка, зрозумійте, що це хобі-проект, і конкретна допомога в його покращенні вітається. Він заснований на інформації, яку я знайшов в Інтернеті та на форумах NVIDIA; я дуже сподіваюся, що він допоможе розвитку домашніх лабораторій. Проект в основному сфокусований на одиночному налаштуванні DGX Spark і повинен працювати на ньому за замовчуванням, але підтримка двох пристроїв вітається.

## Документація

- **[Архітектура та як це працює](docs/architecture.md)** - Розуміння стека, сервісу waker та потоку запитів.
- **[Конфігурація](docs/configuration.md)** - Змінні середовища, налаштування мережі та налаштування waker.
- **[Гід по вибору моделей](docs/models.md)** - Детальний список 29+ підтримуваних моделей, швидкий вибір та сценарії використання.
- **[Інтеграції](docs/integrations.md)** - Посібники для **Cline** (VS Code) та **OpenCode** (Термінальний агент).
- **[Безпека та віддалений доступ](docs/security.md)** - Посилення SSH та налаштування обмеженого перенаправлення портів.
- **[Усунення несправностей та моніторинг](docs/troubleshooting.md)** - Налагодження, логи та вирішення поширених помилок.
- **[Просунуте використання](docs/advanced.md)** - Додавання нових моделей, користувацькі конфігурації та постійна робота.
- **[Нотатки TODO](TODO.md)** - Ідеї про те, що робити далі.

## Швидкий старт

1. **Клонуйте репозиторій**
   ```bash
   git clone <repository-url>
   cd dgx-spark-inference-stack
   ```

2. **Створіть необхідні директорії**
   ```bash
   mkdir -p models vllm_cache_huggingface manual_download/openai_gpt-oss-encodings_fix
   ```

3. **Завантажте необхідні токенізатори (КРИТИЧНО)**
   Стек вимагає ручного завантаження файлів tiktoken для моделей GPT-OSS.
   ```bash
   wget https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken -O manual_download/openai_gpt-oss-encodings_fix/cl100k_base.tiktoken
   wget https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken -O manual_download/openai_gpt-oss-encodings_fix/o200k_base.tiktoken
   ```

4. **Зберіть користувацькі Docker-образи (ОБОВ'ЯЗКОВО)**
   Стек використовує спеціально оптимізовані образи vLLM, які повинні бути зібрані локально для забезпечення максимальної продуктивності.
   *   **Час:** Очікуйте ~20 хвилин на образ.
   *   **Авторизація:** Ви повинні авторизуватися в NVIDIA NGC, щоб отримати базові образи.
       1.  Створіть обліковий запис розробника в [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/) (не повинен знаходитися в санкційній країні).
       2.  Запустіть `docker login nvcr.io` з вашими обліковими даними.
   *   **Команди збірки:**
       ```bash
       # Збірка образу Avarok (Загального призначення) - ПОВИННІ використовувати цей тег для використання локальної версії замість upstream
       docker build -t avarok/vllm-dgx-spark:v11 custom-docker-containers/avarok

       # Збірка образу Christopher Owen (Оптимізований для MXFP4)
       docker build -t christopherowen/vllm-dgx-spark:v12 custom-docker-containers/christopherowen
       ```

5. **Запустіть стек**
   ```bash
   # Запуск тільки шлюзу та waker (моделі запускаються на вимогу)
   docker compose up -d

   # Попереднє створення всіх включених контейнерів моделей (рекомендується)
   docker compose --profile models up --no-start
   ```

6. **Перевірте API**
   ```bash
   # Запит до qwen2.5-1.5b (запуститься автоматично)
   curl -X POST http://localhost:8009/v1/qwen2.5-1.5b-instruct/chat/completions \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer ${VLLM_API_KEY:-63TestTOKEN0REPLACEME}" \
     -d '{
       "model": "qwen2.5-1.5b-instruct",
       "messages": [{"role": "user", "content": "Привіт!"}]
     }'
   ```

## Попередні вимоги
- Docker 20.10+ з Docker Compose
- NVIDIA GPU з підтримкою CUDA та NVIDIA Container Toolkit
- Linux хост (протестовано на Ubuntu)

## Внесок у проект

Pull Requests дуже вітаються. :)
Однак, щоб гарантувати стабільність, я застосовую суворий **Шаблон Pull Request**.

## ⚠️ Відомі проблеми

### Експериментальні моделі (Сумісність з GB10/CUDA 12.1)

Наступні моделі позначені як **експериментальні** через спорадичні збої на DGX Spark (GB10 GPU):

- **Qwen3-Next-80B-A3B-Instruct** - Випадкові збої в шарі лінійної уваги
- **Qwen3-Next-80B-A3B-Thinking** - Та ж проблема

**Основна причина:** GPU GB10 використовує CUDA 12.1, але поточний стек vLLM/PyTorch підтримує тільки CUDA ≤12.0. Це викликає помилки `cudaErrorIllegalInstruction` після кількох успішних запитів.

**Обхідний шлях:** Використовуйте `gpt-oss-20b` або `gpt-oss-120b` для стабільного виклику інструментів, поки не з'явиться оновлений образ vLLM з належною підтримкою GB10.

### Nemotron 3 Nano 30B (NVFP4)

Модель **`nemotron-3-nano-30b-nvfp4`** наразі відключена.
**Причина:** Несумісна з поточною збіркою vLLM на GB10. Потрібна належна підтримка рушія V1 або оновлена реалізація бекенда.


### Підтримка зображень/скріншотів OpenCode в Linux

У OpenCode (термінальний AI-агент) є відома помилка в Linux, коли **зображення з буфера обміну та зображення за шляхом до файлу не працюють** з моделями зору. Модель відповідає "The model you're using does not support image input", хоча VL-моделі працюють коректно через API.

**Основна причина:** Обробка буфера обміну Linux в OpenCode пошкоджує бінарні дані зображення перед кодуванням (використовує `.text()` замість `.arrayBuffer()`). Фактичні дані зображення не надсилаються на сервер.

**Статус:** Схоже, це помилка на стороні клієнта OpenCode. Допомога в розслідуванні/виправленні вітається! Стек інференсу коректно обробляє зображення base64 при правильному надсиланні (перевірено через curl).

**Обхідний шлях:** Використовуйте curl або інші API-клієнти для надсилання зображень безпосередньо в VL-моделі, такі як `qwen2.5-vl-7b`.

### Несумісність Qwen 2.5 Coder 7B та OpenCode

У моделі `qwen2.5-coder-7b-instruct` є суворе обмеження контексту в **32,768 токенів**. Однак OpenCode зазвичай надсилає дуже великі запити (буфер + вхід), що перевищують **35,000 токенів**, що викликає `ValueError` та збої запитів.

**Рекомендація:** Не використовуйте `qwen2.5-coder-7b` з OpenCode для завдань з довгим контекстом. Замість цього використовуйте **`qwen3-coder-30b-instruct`**, яка підтримує **65,536 токенів** контексту та комфортно обробляє великі запити OpenCode.

### Несумісність Llama 3.3 та OpenCode

Модель **`llama-3.3-70b-instruct-fp4`** **не рекомендується для використання з OpenCode**.
**Причина:** Хоча модель коректно працює через API, вона демонструє агресивну поведінку виклику інструментів при ініціалізації специфічними клієнтськими промптами OpenCode. Це призводить до помилок валідації та погіршення користувацького досвіду (наприклад, спроба викликати інструменти відразу після привітання).
**Рекомендація:** Використовуйте замість цього `gpt-oss-20b` або `qwen3-next-80b-a3b-instruct` для сесій OpenCode.

## Подяки

Особлива подяка членам спільноти, які створили оптимізовані Docker-образи, що використовуються в цьому стеку:

- **Thomas P. Braun з Avarok**: За образ vLLM загального призначення (`avarok/vllm-dgx-spark`) з підтримкою non-gated активацій (Nemotron) та гібридних моделей, а також за такі пости, як цей https://blog.avarok.net/dgx-spark-nemotron3-and-nvfp4-getting-to-65-tps-8c5569025eb6.
- **Christopher Owen**: За оптимізований для MXFP4 образ vLLM (`christopherowen/vllm-dgx-spark`), що забезпечує високопродуктивний інференс на DGX Spark.
- **eugr**: За всю роботу над кастомізацією оригінального образу vLLM (`eugr/vllm-dgx-spark`) та чудові пости на форумах NVIDIA.

### Провайдери моделей

Величезне спасибі організаціям, які оптимізують ці моделі для інференсу FP4/FP8:

- **Fireworks AI** (`Firworks`): За широкий спектр оптимізованих моделей, включаючи GLM-4.5, Llama 3.3 та Ministral.
- **NVIDIA**: За Qwen3-Next, Nemotron та стандартні реалізації FP4.
- **RedHat**: За Qwen3-VL та Mistral Small.
- **QuantTrio**: За Qwen3-VL-Thinking.
- **OpenAI**: За моделі GPT-OSS.

## Ліцензія

Цей проект ліцензується відповідно до **Apache License 2.0**. Див. файл [LICENSE](LICENSE) для подробиць.
