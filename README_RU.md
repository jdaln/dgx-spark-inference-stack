# DGX Spark Inference Stack - Дом для ваших LLM!

> **Отказ от ответственности:** Этот документ был переведен искусственным интеллектом и может содержать ошибки.

Ваш Nvidia DGX Spark не должен быть просто еще одним побочным проектом. Начните использовать его! Это стек для инференса на основе Docker для обслуживания больших языковых моделей (LLM) с использованием NVIDIA vLLM с интеллектуальным управлением ресурсами. Этот стек обеспечивает загрузку моделей по требованию с автоматическим выключением при простое, однопользовательское планирование GPU и единый API-шлюз.

Цель проекта — предоставить сервер инференса для вашего дома. После тестирования и добавления новых моделей в течение месяца я решил выпустить его для сообщества. Пожалуйста, поймите, что это хобби-проект, и конкретная помощь в его улучшении приветствуется. Он основан на информации, которую я нашел в Интернете и на форумах NVIDIA; я очень надеюсь, что он поможет развитию домашних лабораторий. Проект в основном сфокусирован на одиночной настройке DGX Spark и должен работать на ней по умолчанию, но поддержка двух устройств приветствуется.

## Документация

- **[Архитектура и как это работает](docs/architecture.md)** - Понимание стека, сервиса waker и потока запросов.
- **[Конфигурация](docs/configuration.md)** - Переменные окружения, настройки сети и настройка waker.
- **[Гид по выбору моделей](docs/models.md)** - Подробный список 29+ поддерживаемых моделей, быстрый выбор и сценарии использования.
- **[Интеграции](docs/integrations.md)** - Руководства для **Cline** (VS Code) и **OpenCode** (Терминальный агент).
- **[Безопасность и удаленный доступ](docs/security.md)** - Усиление SSH и настройка ограниченного перенаправления портов.
- **[Устранение неполадок и мониторинг](docs/troubleshooting.md)** - Отладка, логи и решения распространенных ошибок.
- **[Продвинутое использование](docs/advanced.md)** - Добавление новых моделей, пользовательские конфигурации и постоянная работа.
- **[Заметки TODO](TODO.md)** - Идеи о том, что делать дальше.

## Быстрый старт

1. **Клонируйте репозиторий**
   ```bash
   git clone <repository-url>
   cd dgx-spark-inference-stack
   ```

2. **Создайте необходимые директории**
   ```bash
   mkdir -p models vllm_cache_huggingface manual_download/openai_gpt-oss-encodings_fix
   ```

3. **Скачайте необходимые токенизаторы (КРИТИЧНО)**
   Стек требует ручной загрузки файлов tiktoken для моделей GPT-OSS.
   ```bash
   wget https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken -O manual_download/openai_gpt-oss-encodings_fix/cl100k_base.tiktoken
   wget https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken -O manual_download/openai_gpt-oss-encodings_fix/o200k_base.tiktoken
   ```

4. **Соберите пользовательские Docker-образы (ОБЯЗАТЕЛЬНО)**
   Стек использует специально оптимизированные образы vLLM, которые должны быть собраны локально для обеспечения максимальной производительности.
   *   **Время:** Ожидайте ~20 минут на образ.
   *   **Авторизация:** Вы должны авторизоваться в NVIDIA NGC, чтобы получить базовые образы.
       1.  Создайте аккаунт разработчика в [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/) (не должен находиться в санкционной стране).
       2.  Запустите `docker login nvcr.io` с вашими учетными данными.
   *   **Команды сборки:**
       ```bash
       # Сборка образа Avarok (Общего назначения) - ДОЛЖНЫ использовать этот тег для использования локальной версии вместо upstream
       docker build -t avarok/vllm-dgx-spark:v11 custom-docker-containers/avarok

       # Сборка образа Christopher Owen (Оптимизирован для MXFP4)
       docker build -t christopherowen/vllm-dgx-spark:v12 custom-docker-containers/christopherowen
       ```

5. **Запустите стек**
   ```bash
   # Запуск только шлюза и waker (модели запускаются по требованию)
   docker compose up -d

   # Предварительное создание всех включенных контейнеров моделей (рекомендуется)
   docker compose --profile models up --no-start
   ```

6. **Проверьте API**
   ```bash
   # Запрос к qwen2.5-1.5b (запустится автоматически)
   curl -X POST http://localhost:8009/v1/qwen2.5-1.5b-instruct/chat/completions \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer ${VLLM_API_KEY:-63TestTOKEN0REPLACEME}" \
     -d '{
       "model": "qwen2.5-1.5b-instruct",
       "messages": [{"role": "user", "content": "Привет!"}]
     }'
   ```

## Предварительные требования
- Docker 20.10+ с Docker Compose
- NVIDIA GPU с поддержкой CUDA и NVIDIA Container Toolkit
- Linux хост (протестировано на Ubuntu)

## Вклад в проект

Pull Requests очень приветствуются. :)
Однако, чтобы гарантировать стабильность, я применяю строгий **Шаблон Pull Request**.

## ⚠️ Известные проблемы

### Экспериментальные модели (Совместимость с GB10/CUDA 12.1)

Следующие модели помечены как **экспериментальные** из-за спорадических сбоев на DGX Spark (GB10 GPU):

- **Qwen3-Next-80B-A3B-Instruct** - Случайные сбои в слое линейного внимания
- **Qwen3-Next-80B-A3B-Thinking** - Та же проблема

**Основная причина:** GPU GB10 использует CUDA 12.1, но текущий стек vLLM/PyTorch поддерживает только CUDA ≤12.0. Это вызывает ошибки `cudaErrorIllegalInstruction` после нескольких успешных запросов.

**Обходной путь:** Используйте `gpt-oss-20b` или `gpt-oss-120b` для стабильного вызова инструментов, пока не появится обновленный образ vLLM с надлежащей поддержкой GB10.

### Nemotron 3 Nano 30B (NVFP4)

Модель **`nemotron-3-nano-30b-nvfp4`** в настоящее время отключена.
**Причина:** Несовместима с текущей сборкой vLLM на GB10. Требуется надлежащая поддержка движка V1 или обновленная реализация бэкенда.


### Поддержка изображений/скриншотов OpenCode в Linux

У OpenCode (терминальный AI-агент) есть известная ошибка в Linux, когда **изображения из буфера обмена и изображения по пути к файлу не работают** с моделями зрения. Модель отвечает "The model you're using does not support image input", хотя VL-модели работают корректно через API.

**Основная причина:** Обработка буфера обмена Linux в OpenCode повреждает бинарные данные изображения перед кодированием (использует `.text()` вместо `.arrayBuffer()`). Фактические данные изображения не отправляются на сервер.

**Статус:** Похоже, это ошибка на стороне клиента OpenCode. Помощь в расследовании/исправлении приветствуется! Стек инференса корректно обрабатывает изображения base64 при правильной отправке (проверено через curl).

**Обходной путь:** Используйте curl или другие API-клиенты для отправки изображений напрямую в VL-модели, такие как `qwen2.5-vl-7b`.

### Несовместимость Qwen 2.5 Coder 7B и OpenCode

У модели `qwen2.5-coder-7b-instruct` есть строгое ограничение контекста в **32,768 токенов**. Однако OpenCode обычно отправляет очень большие запросы (буфер + вход), превышающие **35,000 токенов**, что вызывает `ValueError` и сбои запросов.

**Рекомендация:** Не используйте `qwen2.5-coder-7b` с OpenCode для задач с длинным контекстом. Вместо этого используйте **`qwen3-coder-30b-instruct`**, которая поддерживает **65,536 токенов** контекста и комфортно обрабатывает большие запросы OpenCode.

### Несовместимость Llama 3.3 и OpenCode

Модель **`llama-3.3-70b-instruct-fp4`** **не рекомендуется для использования с OpenCode**.
**Причина:** Хотя модель корректно работает через API, она демонстрирует агрессивное поведение вызова инструментов при инициализации специфическими клиентскими промптами OpenCode. Это приводит к ошибкам валидации и ухудшению пользовательского опыта (например, попытка вызвать инструменты сразу после приветствия).
**Рекомендация:** Используйте вместо этого `gpt-oss-20b` или `qwen3-next-80b-a3b-instruct` для сессий OpenCode.

## Благодарности

Особая благодарность членам сообщества, создавшим оптимизированные Docker-образы, используемые в этом стеке:

- **Thomas P. Braun из Avarok**: За образ vLLM общего назначения (`avarok/vllm-dgx-spark`) с поддержкой non-gated активаций (Nemotron) и гибридных моделей, а также за такие посты, как этот https://blog.avarok.net/dgx-spark-nemotron3-and-nvfp4-getting-to-65-tps-8c5569025eb6.
- **Christopher Owen**: За оптимизированный для MXFP4 образ vLLM (`christopherowen/vllm-dgx-spark`), обеспечивающий высокопроизводительный инференс на DGX Spark.
- **eugr**: За всю работу над кастомизацией оригинального образа vLLM (`eugr/vllm-dgx-spark`) и отличные посты на форумах NVIDIA.

### Провайдеры моделей

Огромное спасибо организациям, оптимизирующим эти модели для инференса FP4/FP8:

- **Fireworks AI** (`Firworks`): За широкий спектр оптимизированных моделей, включая GLM-4.5, Llama 3.3 и Ministral.
- **NVIDIA**: За Qwen3-Next, Nemotron и стандартные реализации FP4.
- **RedHat**: За Qwen3-VL и Mistral Small.
- **QuantTrio**: За Qwen3-VL-Thinking.
- **OpenAI**: За модели GPT-OSS.

## Лицензия

Этот проект лицензируется в соответствии с **Apache License 2.0**. См. файл [LICENSE](LICENSE) для подробностей.
